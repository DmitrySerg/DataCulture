{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://hsto.org/files/256/a5d/ed0/256a5ded03274e0f87ccf97164c31c35.png)\n",
    "\n",
    "\n",
    "# Регрессия - моя профессия\n",
    "\n",
    "\n",
    "\n",
    "Вспомним: задача, где надо спрогнозировать класс называется классификацией (неожиданное название). Задача, где надо спрогнозировать непрерывную переменную называется регрессией. Пример непрерывной переменной: цена на квартиры. Именно её прогнозированием мы сегодня и займёмся. Поехали!\n",
    "\n",
    "\n",
    "## 1. Предобработка данных \n",
    "\n",
    "Начнём наш крестовый поход за дешёвой недвижимостью с предобработки данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd               # уже знакомый вам пакет для работы с таблицами\n",
    "import numpy as np                # смутно знакомый вам пакет для работы с матрицами\n",
    "import matplotlib.pyplot as plt   # уже смутно знакомый вам пакет для картинок :3\n",
    "import seaborn as sns             # ещё один пакет для картинок \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/flat.csv', sep='\\t', index_col='n')  # подгружаем табличку \n",
    "print('Размер выборки:', df.shape)                     # смотрим на её размеры \n",
    "df.head( ) # Смотрим что лежит в табличке "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Описание переменных:__\n",
    "\n",
    "```\n",
    "n – номер квартиры по порядку\n",
    "price – цена квартиры в $1000\n",
    "totsp – общая площадь квартиры, кв.м.\n",
    "livesp жилая площадь квартиры, кв.м.\n",
    "kitsp – площадь кухни, кв.м.\n",
    "dist – расстояние от центра в км.\n",
    "metrdist – расстояние до метро в минутах\n",
    "walk – 1 – пешком от метро, 0 – на транспорте\n",
    "brick 1 – кирпичный, монолит ж/б, 0 – другой\n",
    "floor 1 – этаж кроме первого и последнего, 0 – иначе.\n",
    "code – число от 1 до 8, при помощи которого мы группируем наблюдения по\n",
    "подвыборкам:\n",
    "1. Наблюдения сгруппированы на севере, вокруг Калужско-Рижской линии\n",
    "метрополитена\n",
    "2. Север, вокруг Серпуховско-Тимирязевской линии метрополитена\n",
    "3. Северо-запад, вокруг Замоскворецкой линии метрополитена\n",
    "4. Северо-запад, вокруг Таганско-Краснопресненской линии метрополитена\n",
    "5. Юго-восток, вокруг Люблинской линии метрополитена\n",
    "6. Юго-восток, вокруг Таганско-Краснопресненской линии метрополитена\n",
    "7. Восток, вокруг Калиниской линии метрополитена\n",
    "8. Восток, вокруг Арбатско-Покровской линии метрополитена\n",
    "```\n",
    "\n",
    "\n",
    "Какие переменный категориальные? Какие непрерывные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info() # Аосмотрим на информацию по типам переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # посмотрим на то есть ли в переменных пропуски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков нет. Это хорошая новость. А что делать, если бы они были? Вспоминайте, что мы делали в прошлый раз с Титаником! \n",
    "\n",
    "Посмотрим как выглядит распределение цен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У распределения цен есть проблема - очень длинный хвост. В выборке встречаются квартиры с довольно большой стоимостью. Такие наблюдения называются выбросами. С ними нужно бороться, иначе наша модель подстроится под них. Давайте сгладим распределние цен, прологарифмировав его. Так довольно часто поступают с целевой переменной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = np.log(df['price'])\n",
    "df.price.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на рспределение всех остальных признаков. Давайте обсудим, что интересного видно на картинках!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('price',axis=1).hist(figsize=(20, 12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме гистограммок имеет смысл взглянуть на матрицу корреляций. Что видно на ней? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё давайте посмотрим на облака рассеивания и успокоимся с картинками. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распределение выглядит более приятно. Займёмся предобработкой категориальных переменных. Как вы помните из предыдущего блокнота про титаник, это делается одним горячим кодированием (One Hot Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделали OHE для категориальной переменной\n",
    "df_categor = pd.get_dummies(df['code'], drop_first=True, prefix='code')\n",
    "\n",
    "# Объединили назад наши таблички\n",
    "df = pd.concat([df.drop('code',axis=1),df_categor], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим данные на тренировочные и тестовые! $30\\%$ данных откладываем для тестирования качества модели. Остальные $70\\%$ берём для обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний предобрабатывательский штрих это скалирование (стандартизация непрерывных переменных). Давайте вспомним зачем его делают. Обычно, когда обучают модель, хотят минимизировать ошибку, которую она допускает. Чаще всего эту функцию минимизируют численно. Если переменные измерены в разных шкалах (что-то в тоннах, что-то в годах и тд), алгоритм может при обучении заблудиться. Хорошо бы направить его и подтолкнуть в нужном направлении. Таким толчком является стандартизация переменных. \n",
    "\n",
    "Из каждой переменной вычетают среднее и делят на стандартное отклонение. Это очищает переменные от своих уникальных шкал и упрощает путь алгоритма к оптимальной точке. Обычно стандартное отклонение и среднее для скалирования оценивают на обучающей выборке. К тестовой применяют уже оценённый результат. Это позволяет не подглядывать в тестовую часть и не улучшать за счёт этого подглядывания прогнозы. Подглядывать - нечестно! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# объявили скалировщик!\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# учим скалировщик скалировать все переменны на трэйне\n",
    "scaler.fit(df_train[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist']]) \n",
    "\n",
    "# Примечание: когда будете делать последнюю домашку, выждите когда мама или бабушка\n",
    "# спросит а что это ты делаешь? Выдайте ей, что вы учите на трэйне скалировщик скалировать, \n",
    "# чтобы стохастический градиентый спуск быстрее обучил вашу регрессию.\n",
    "# Запомните их реакцию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Применяем скалировщик к трэйну\n",
    "df_train_scale = scaler.transform(df_train[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist']])\n",
    "\n",
    "# Применяем скалирвощик к тесту \n",
    "df_test_scale = scaler.transform(df_test[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Заменяем значения на роскалированные \n",
    "df_train[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist']] = df_train_scale\n",
    "df_test[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist']] = df_test_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все непрерывные переменные теперь выглядят проскалированными :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape) # Посмотрим на размеры трэйна и теста \n",
    "print(df_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Вытаскиваем цены и параметры квартир по разным переменным для удобства \n",
    "\n",
    "y_train = df_train.price \n",
    "y_test = df_test.price \n",
    "\n",
    "X_train = df_train.drop('price', axis=1).get_values()\n",
    "X_test = df_test.drop('price', axis=1).get_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Константный прогноз "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг первый. Построим константный прогноз. Будем говорить, что стоимость любой квартиры равна среднему значению. Это самый глупый прогноз, который мы можем сделать. Мы будм сравнивать с ним прогнозы более сложных моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.mean(y_train)                     # посчитали среднее \n",
    "y_pred_naive = np.ones(len(y_test)) * y_mean  # спрогнозировали им цену всех квартир в тестовой выборке\n",
    "y_pred_naive[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Метрики качества для регрессии \n",
    "\n",
    "Мы сделали выше прогноз. Теперь мы хотим понять насколько он хороший. Для этого обычно используют метрики. Посмотрим на несколько таких метрик. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics  # подгружаем метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первой метрикой, с которой мы познакомимся, будет MAE (mean absolute error), средняя абсолютная ошибка. Она вычисляется следующим образом: \n",
    "\n",
    "$$ MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|. $$\n",
    "\n",
    "Если мы спрогнозировали, что квартира стоит 20 рублей, а она стоила 10 рублей, мы ошиблись на |10 - 20| = 10 рублей. Средняя абсолютная ошибка - это средняя сумма рублей, на которую мы облажались. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомню, что мы прогнозируем логарифм цены, нам так удобнее. Ошибка выше считается в логарифмах. Если мы хотим посмотреть на ошибку в долларах, надо взять экспоненту от цен. Ниже мы можем увидеть, что в среднем ошибаемся на тридцать с лишним тысяч долларов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(np.exp(y_test), np.exp(y_pred_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй метрикой является MSE (mean squared error), средняя квадратичная ошибка. Она вычисляется как \n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.$$\n",
    "\n",
    "Смысл этой ошибки в том, чтобы штрафовать за большие ошибки сильнее, чем за маленькие. Если мы ошиблись на 5 долларов, то в ошибку войдёт 25. Если мы ошиблись на 10 долларов, то в ошибку войдёт 100. Чем выше ошибка, тем сильнее штраф. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии перейдём к долларам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(np.exp(y_test), np.exp(y_pred_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось многовато, не находите? Всё дело в том, что это не просто доллары, это квадратные доллары. Мы же суммировали квдараты. Неплохо было бы вернутся к обычным долларам. Для этого надо взять из MSE квадратный корень. Тогда получится новая ошибка, RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(np.exp(y_test), np.exp(y_pred_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка в среднем более чем на 60 тысяч долларов. Так как более большие ошибки входят с более большим весом, вполне логично, что RMSE получилось больше, чем MAE. \n",
    "\n",
    "Часто для нас принципиальным является не то, на сколько денег мы ошиблись, а то на сколько процентов мы ошиблись. Метрика, которая отлавливает процентную ошибку, называется MAPE (mean absolute percentage error), средняя абсолютная процентная ошибка. \n",
    "\n",
    "$$\n",
    "MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{y_i}\n",
    "$$\n",
    "\n",
    "Она часто применяется в следующих задачах: например, вы прогнозируете спрос, и вам принципиально, на сколько процентов вы ошиблись, а не абсолютное значение. Если вы предсказали  один, а в реальности было  десять - это не то же самое, что вы предсказали  тысяча, а в реальности было  тысяча  девять. С точки зрения МАЕ или MSE, это две совершенно одинаковые ошибки. А если вас интересует, сколько в среднем на сколько процентов вы ошибаетесь, то это отражает МАРЕ.\n",
    "\n",
    "Её нам придётся реальзовать самостоятельно. Благо, это не очень трудно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mean_absolute_percentage_error(y_test, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средняя ошибка примерно на $5\\%$ от цены. \n",
    "\n",
    "Последняя метрика, с которой нам нужно познакомиться, это коэффициент детерминации, $R^2$. Он отражает то, какую долю дисперсии объясняемой переменной мы объяснили с помощью нашей модели:\n",
    "\n",
    "$$ R^2 =1- \\frac{ \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{ \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2} $$\n",
    "\n",
    "Эту метрику очень сильно любят консалтеры и аудиторы, потому что только её они и знают. На самом деле в ней нет ничего хорошего. При добавлении в модель новых переменных она всегда растёт. У неё есть ещё несколько тонких математических недостатков, о которых вы можете узнать из книг. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закинем все метрики в одну общую функцию, чтобы было удобно их печатать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metrics(y_test,y_pred):\n",
    "    print('MAE:', metrics.mean_absolute_error(np.exp(y_test), np.exp(y_pred)))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(np.exp(y_test), np.exp(y_pred))))\n",
    "    print('R2:',  metrics.r2_score(y_test, y_pred))\n",
    "    print('MAPE:', mean_absolute_percentage_error(y_test, y_pred))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, y_pred_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Строим нашу первую регрессию!\n",
    "\n",
    "Пришло время построить линейную регрессию! Эта модель говорит, что цена на квартиру формируется в результате суммирования тех характеристик, которыми она обладает с какими-то весами\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_n x_n.$$\n",
    "\n",
    "Например, если мы оценили модель и у нас получилось, что \n",
    "\n",
    "$$ price = 10000 + 20 \\cdot totsp,$$\n",
    "\n",
    "то это означает, что средняя стоимость квартиры равна 10 тыс. долларам. При этом каждый дополнительный метр общей площади квартиры делает её дороже на 20 рублей. \n",
    "\n",
    "Для того, чтобы обучить регрессию минимизируют одну из метрик, перечисленных в прошлом разделе. В базовой комплектации регрессии это делают с MSE. Такая модель обладает огромным количеством няшных статистических свойств. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Объявили модель\n",
    "model_regression = LinearRegression()\n",
    "\n",
    "# Обучили модель на тренировочной выборке \n",
    "model_regression.fit(X_train, y_train)\n",
    "\n",
    "# Сделали прогнозы на тестовой выборке \n",
    "y_pred_regr = model_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество прогнозов. Мы стали ошибаться меньше, чем раньше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, какие признаки вносят в цену наибольший вклад. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame({\"feature\": df.drop('price',axis=1).columns, \n",
    "                                  \"importance\": model_regression.coef_})\n",
    "\n",
    "featureImportance.set_index('feature', inplace=True)\n",
    "featureImportance.sort_values([\"importance\"], ascending=False, inplace=True)\n",
    "featureImportance[\"importance\"].plot('bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Строим Lasso-регрессию "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим более сложную модель, LASSO-регрейссию. Фишка этой модели в том, что она зануляет лишние коэффиценты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Объявили модель\n",
    "model_simplelasso = Lasso()\n",
    "\n",
    "# Обучили модель на тренировочной выборке \n",
    "model_simplelasso.fit(X_train, y_train)\n",
    "\n",
    "# Сделали прогнозы на тестовой выборке \n",
    "y_pred_lasso = model_simplelasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на важность факторов для стоимости квартиры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame({\"feature\": df.drop('price',axis=1).columns, \n",
    "                                  \"importance\": model_simplelasso.coef_})\n",
    "\n",
    "featureImportance.set_index('feature', inplace=True)\n",
    "featureImportance.sort_values([\"importance\"], ascending=False, inplace=True)\n",
    "featureImportance[\"importance\"].plot('bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важных факторов нет, всё занулилось. Метрики качества такие же, как при константном прогнозе. Почему такое произошло? Дело в том, что у модели есть гиперпараметр - сила зануления. И его нужно подбирать методом перебора. В нашей модели он стоял слишком большим. Давайте попробуем подобрать этот параметр. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Подбор гиперпараметра для Lasso-регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем делать перебор следующим способом: дробим тренировочную выборку на пять частей. На четырёх учим модель, на пятой прогнозируем. Смотрим на качество. И так по очереди выделяем для прогноза каждую из 5 частичек. Потом качество прогноза усредняем. Из титаника вы можете помнить, что такая стратегия называется скользящим контролем или кросс-валидацией. Для какого параметра из решётки качетство получится наибольшим, тот мы и оставим. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Решётака для перебора параметра \n",
    "param_grid = {'alpha': [0.001, 0.01, 0.05, 0.1, 0.5, 0.8, 1, 5, 10]}\n",
    "\n",
    "# Объявили модель \n",
    "model_lasso = Lasso() \n",
    "\n",
    "# Объявили перебор \n",
    "grid_cv_lasso = GridSearchCV(model_lasso, param_grid, cv = 5)\n",
    "grid_cv_lasso.fit(X_train, y_train)\n",
    "print('Лучшее значение параметра:', grid_cv_lasso.best_params_)\n",
    "\n",
    "# Сделали прогнозы\n",
    "y_pred_lasso = grid_cv_lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При параметре 0.001 качество у прогнозов получилось самым хорошим. Его и берём. Посмотрим на важность переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame({\"feature\": df.drop('price',axis=1).columns, \n",
    "                                  \"importance\": grid_cv_lasso.best_estimator_.coef_})\n",
    "\n",
    "featureImportance.set_index('feature', inplace=True)\n",
    "featureImportance.sort_values([\"importance\"], ascending=False, inplace=True)\n",
    "featureImportance[\"importance\"].plot('bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на качество модели. Оно оказывается близким к обычной регрессии. Судя по всему у нас в выборке нет лишних переменных и занулять нечего. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Случайный лес\n",
    "\n",
    "Попробуем более сложную модель, случайный лес. Мы обсуждали его в прошлый раз. В ходе его обучения будут строится деревья. Каждое из них будет выдавать прогноз, потом мы будем по всему лесу прогнозы усреднять. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Объявили лес из 100 деревьев\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Обучили лес \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Сделали по лесу прогнозы \n",
    "y_pred_forest = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на важность факторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = pd.DataFrame({\"feature\": df.drop('price',axis=1).columns, \n",
    "                                  \"importance\": rf.feature_importances_})\n",
    "\n",
    "featureImportance.set_index('feature', inplace=True)\n",
    "featureImportance.sort_values([\"importance\"], ascending=False, inplace=True)\n",
    "featureImportance[\"importance\"].plot('bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сравниваем модели между собой "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое хорошее качество дал случайный лес. Недалеко от него ушла обычная линейная регрессия. Наверное, уместно было бы при прогнозировании цены на дома использовать именно её. Она обучается намного быстрее и легче леса. К тому же она интерпретируема. На этом на сегодня всё. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
